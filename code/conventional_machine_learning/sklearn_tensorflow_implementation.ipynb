{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "# Index\n",
    "\n",
    "## 1. Enviroment Preparation\n",
    "#### &nbsp;&nbsp;&nbsp;&nbsp;1.1. Import Libraries\n",
    "#### &nbsp;&nbsp;&nbsp;&nbsp;1.2. Define Error and Loss Fuction\n",
    "#### &nbsp;&nbsp;&nbsp;&nbsp;1.3. Define Data Preprosessing Function\n",
    "#### &nbsp;&nbsp;&nbsp;&nbsp;1.4. Define General Algorithms and able-to-be-looped Objects\n",
    "\n",
    "## 2. Data Preparation\n",
    "#### &nbsp;&nbsp;&nbsp;&nbsp;2.1. Define Data Preprosessing Function\n",
    "#### &nbsp;&nbsp;&nbsp;&nbsp;2.2. Screening for Suitable Data Preprossessing Techniques\n",
    "\n",
    "## 3. Optimization of Hyperparameters\n",
    "#### &nbsp;&nbsp;&nbsp;&nbsp;3.1. Find the Best n_estimators/n_neighbors for Preliminary Search Space Reduction\n",
    "#### &nbsp;&nbsp;&nbsp;&nbsp;3.2. Use GridSearchCV to Look Up Hyperparameters except NN and MLP\n",
    "#### &nbsp;&nbsp;&nbsp;&nbsp;3.3. (Neural Network Intelligence is Implement in  Another File)\n",
    "## 4. 10-Fold CV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "### Separate Line ###"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "# 1. Enviroment Preparation\n",
    "## 1.1 Import Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# sklearn\n",
    "from sklearn.preprocessing import normalize, power_transform, binarize, maxabs_scale, minmax_scale, quantile_transform\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn import ensemble\n",
    "from sklearn import svm\n",
    "from sklearn import neighbors\n",
    "from sklearn import neural_network\n",
    "\n",
    "# Tensorflow\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers\n",
    "from tensorflow.keras.callbacks import TensorBoard\n",
    "\n",
    "# XGBoost\n",
    "from xgboost import XGBRegressor\n",
    "\n",
    "# Matplotlib & Seaborn\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.lines as mlines\n",
    "\n",
    "\n",
    "# Data structure and Math\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from scipy.stats import pearsonr\n",
    "from scipy.stats import gaussian_kde\n",
    "import math"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "## 1.2 Define Error and Loss Fuction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# Define RMSE\n",
    "def rmse(predictions, targets):\n",
    "    return np.sqrt(((predictions - targets) ** 2).mean())\n",
    "\n",
    "\n",
    "# Define RMSE Loss\n",
    "def rmse_loss(y_test, y_pred):\n",
    "    loss = tf.sqrt(tf.reduce_mean(tf.square(y_test - y_pred)))\n",
    "    return loss"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "## 1.3. Define Data Preprosessing Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "data_regularization = ['power_transform', 'quantile_transform',\n",
    "                       'maxabs_scale', 'minmax_scale',\n",
    "                       'binarize', 'l2']\n",
    "\n",
    "\n",
    "def prepare_data(X, prepro):\n",
    "    if prepro == 'l2':\n",
    "        return normalize(X, norm='l2')\n",
    "    if prepro == 'power_transform':\n",
    "        return power_transform(X)\n",
    "    if prepro == 'binarize':\n",
    "        return binarize(X)\n",
    "    if prepro == 'maxabs_scale':\n",
    "        return maxabs_scale(X)\n",
    "    if prepro == 'minmax_scale':\n",
    "        return minmax_scale(X)\n",
    "    if prepro == 'quantile_transform':\n",
    "        return quantile_transform(X)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "## 1.4. Define General Algorithms and able-to-be-looped Objects"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# Define a code that run through algorithms of sklearn and result in the same format of output data\n",
    "# sklearn algorithms and XGB\n",
    "regressor_objects = {\n",
    "                     # Ensemble regressors\n",
    "                     'RF': ensemble.RandomForestRegressor(n_jobs=-1),\n",
    "                     'ET': ensemble.ExtraTreesRegressor(n_jobs=-1),\n",
    "                     'Bagging': ensemble.BaggingRegressor(n_jobs=-1),\n",
    "                     # SVR and k-NN\n",
    "                     'SVR': svm.SVR(),\n",
    "                     'kNN': neighbors.KNeighborsRegressor(n_jobs=-1),\n",
    "                     # XGB\n",
    "                     'XGB': XGBRegressor(n_jobs=-1),\n",
    "                     # plain NN\n",
    "                     'MLP': neural_network.MLPRegressor((256,256,256))\n",
    "                     }\n",
    "\n",
    "# define a general sklearn machine learning model\n",
    "\n",
    "def ml_model(splited_data,regressor_object):\n",
    "    X_train, X_test, y_train, y_test = \\\n",
    "        splited_data[0], splited_data[1], splited_data[2], splited_data[3]\n",
    "    regressor_object.fit(X_train, y_train)\n",
    "\n",
    "    train_prediction = regressor_object.predict(X_train)\n",
    "    R2_train = np.square(pearsonr(y_train, train_prediction)[0])\n",
    "    RMSE_train = rmse(y_train, train_prediction)\n",
    "\n",
    "    test_prediction = regressor_object.predict(X_test)\n",
    "    R2_test = np.square(pearsonr(y_test, test_prediction)[0])\n",
    "    RMSE_test = rmse(y_test, test_prediction)\n",
    "\n",
    "    return [R2_train, RMSE_train, R2_test, RMSE_test, test_prediction]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "# 2. Data Preparation\n",
    "## 2.1. Import Data for Universal Usage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "data = pd.read_csv(r'input_csv_path')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "## 2.2 Screening for the Best Datapreparation Technique"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "columns = ['Regressor', 'Pearson R', 'Sigma R', 'RMSE', 'Sigma RMSE', 'Fold', 'Preprocessing Technique']\n",
    "metrics = pd.DataFrame(columns=columns)\n",
    "for technique in data_regularization:\n",
    "    X = prepare_data(data.iloc[:, 5:], prepro=technique)\n",
    "    y = data['IE']\n",
    "    kf_num = 10\n",
    "    kf = KFold(n_splits=kf_num)\n",
    "    kf.get_n_splits(X)\n",
    "\n",
    "    print('Now Examing {}'.format(technique))\n",
    "    for regressor, regressor_object in regressor_objects.items():\n",
    "        r2_list = []\n",
    "        rmse_list = []\n",
    "        fold_number = 0\n",
    "        for train_index, test_index in kf.split(X):\n",
    "            fold_number += 1\n",
    "            X_train, X_test = X[train_index], X[test_index]\n",
    "            y_train, y_test = y[train_index], y[test_index]\n",
    "            splited_data = [X_train, X_test, y_train, y_test]\n",
    "            score = ml_model(splited_data, regressor_object)\n",
    "            r2_list.append(score[2])\n",
    "            rmse_list.append(score[3])\n",
    "\n",
    "        r_mean = np.mean(np.array(r2_list))\n",
    "        r_sigma = np.std(np.array(r2_list))\n",
    "        rmse_mean = np.mean(np.array(rmse_list))\n",
    "        rmse_sigma = np.std(np.array(rmse_list))\n",
    "        new_row = pd.DataFrame([[regressor, r_mean, r_sigma, rmse_mean, rmse_sigma, fold_number, technique]], columns=columns)\n",
    "        metrics = metrics.append(new_row, ignore_index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "plt.figure(figsize=(8,6))\n",
    "colors = ['b', 'y', 'g', 'r', 'cyan', 'brown']\n",
    "i = 0\n",
    "for technique in data_regularization:\n",
    "    plt.plot(metrics[metrics['Preprocessing Technique']==technique]['Regressor'],metrics[metrics['Preprocessing Technique']==technique]['RMSE'],'m.-',c=colors[i])\n",
    "    i += 1\n",
    "\n",
    "plt.ylim([0.4, 0.9])\n",
    "plt.legend(data_regularization)\n",
    "plt.tight_layout()\n",
    "plt.savefig(r'output_png_path', dpi=328)\n",
    "plt.show()\n",
    "metrics"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "# 3. Optimization of Hyperparameters\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "#### Define Ploting Fuction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "def plot_bar(metric, metric_name, n_param, n_param_name, algorithms_name):\n",
    "    plt.bar(x=n_param, height=metric, color='b', width=0.7)\n",
    "    plt.ylabel(metric_name)\n",
    "    plt.xlabel(n_param_name)\n",
    "    plt.xlim([n_param[0]-1, n_param[-1]+1])\n",
    "    plt.title('The {} as the Increasing of {} for {}'.format(metric_name, n_param_name,algorithms_name))\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "## 3.1. N_estimators / N_neighbors Selections for Full Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "X = prepare_data(data.iloc[:, 5:], prepro='quantile_transform')\n",
    "y = data['IE']\n",
    "kf_num = 10\n",
    "kf = KFold(n_splits=kf_num)\n",
    "kf.get_n_splits(X)\n",
    "cv_df = pd.DataFrame(columns=['Regressor', 'R^2', 'RMSE', 'Fold Number'])\n",
    "\n",
    "n_estrs = np.arange(1, 101, 1).tolist()\n",
    "n_neibs = np.arange(1,21,1).tolist()\n",
    "\n",
    "estimators_type_algorithms = ['RF', 'ET', 'Bagging','XGB', 'kNN']\n",
    "\n",
    "for regressor in estimators_type_algorithms:\n",
    "    x_lable = 'n_estimators'\n",
    "    if regressor == 'kNN':\n",
    "        n_estrs = n_neibs\n",
    "        x_lable = 'n_neighbors'\n",
    "\n",
    "    rmse_list = []\n",
    "    r2_list = []\n",
    "    for n_est in n_estrs:\n",
    "        fold_number = 0\n",
    "        rmse_mean = 0\n",
    "        r2_mean = 0\n",
    "        for train_index, test_index in kf.split(X):\n",
    "            fold_number += 1\n",
    "            X_train, X_test = X[train_index], X[test_index]\n",
    "            y_train, y_test = y[train_index], y[test_index]\n",
    "            splited_data = [X_train, X_test, y_train, y_test]\n",
    "            if regressor == 'RF':\n",
    "                metrics = ml_model(splited_data, ensemble.RandomForestRegressor(n_estimators=n_est, n_jobs=-1))\n",
    "            elif regressor == 'ET':\n",
    "                metrics = ml_model(splited_data, ensemble.ExtraTreesRegressor(n_estimators=n_est, n_jobs=-1))\n",
    "            elif regressor == 'Bagging':\n",
    "                metrics = ml_model(splited_data, ensemble.BaggingRegressor(n_estimators=n_est, n_jobs=-1))\n",
    "            elif regressor == 'kNN':\n",
    "                metrics = ml_model(splited_data,  neighbors.KNeighborsRegressor(n_neighbors=n_est ,n_jobs=-1))\n",
    "            elif regressor == 'XGB':\n",
    "                metrics = ml_model(splited_data, XGBRegressor(n_estimators=n_est ,n_jobs=-1))\n",
    "            r2_mean += metrics[2]\n",
    "            rmse_mean += metrics[3]\n",
    "        r2_list.append(r2_mean/kf_num)\n",
    "        rmse_list.append(rmse_mean/kf_num)\n",
    "\n",
    "    r2_min = min(r2_list)\n",
    "    rmse_min = min(rmse_list)\n",
    "    n_est_rmse_min = n_estrs[rmse_list.index(rmse_min)]\n",
    "    print('The Best {} for {} is {} with R2={} and RMSE={}.'.format(x_lable, regressor, n_est_rmse_min, r2_min, rmse_min))\n",
    "    plot_bar(r2_list, 'R2', n_estrs, x_lable, regressor)\n",
    "    plot_bar(rmse_list, 'RMSE', n_estrs, x_lable, regressor)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "## 3.2. Use GridSearchCV to Look Up Hyperparameters except NN and MLP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "updated_regressor_objects = {\n",
    "                     # Ensemble regressors\n",
    "                     'RF': ensemble.RandomForestRegressor(n_estimators=80, n_jobs=-1),\n",
    "                     'ET': ensemble.ExtraTreesRegressor(n_estimators=90, n_jobs=-1),\n",
    "                     'Bagging': ensemble.BaggingRegressor(n_estimators=95, n_jobs=-1),\n",
    "                     # SVR and k-NN\n",
    "                     'SVR': svm.SVR(),\n",
    "                     'kNN': neighbors.KNeighborsRegressor(n_neighbors=8, n_jobs=-1),\n",
    "                     # XGB\n",
    "                     'XGB': XGBRegressor(n_estimators=100, n_jobs=-1),\n",
    "                     # plain NN\n",
    "                     #'MLP': neural_network.MLPRegressor()\n",
    "                     }\n",
    "\n",
    "X = prepare_data(data.iloc[:,5:], prepro='quantile_transform')\n",
    "y = data['IE']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "RF_param = {'min_samples_split': [2, 3, 4, 5],\n",
    "            'min_samples_leaf': [1, 2, 3, 4],\n",
    "            'max_features': ['auto', 'sqrt', 'log2'],\n",
    "            }\n",
    "\n",
    "RF_grid = GridSearchCV(updated_regressor_objects['RF'],\n",
    "                       param_grid=RF_param,\n",
    "                       cv=10)\n",
    "RF_grid.fit(X ,y)\n",
    "\n",
    "print('Best Parameter of RF is ', RF_grid.best_params_)\n",
    "print('Best Score of RF is ', RF_grid.best_score_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "ET_param = {'min_samples_split': [2, 3, 4, 5],\n",
    "            'min_samples_leaf': [1, 2, 3, 4],\n",
    "            'max_features': ['auto', 'sqrt', 'log2'],\n",
    "            }\n",
    "\n",
    "ET_grid = GridSearchCV(updated_regressor_objects['ET'],\n",
    "                       param_grid=ET_param,\n",
    "                       cv=10)\n",
    "ET_grid.fit(X ,y)\n",
    "\n",
    "print('Best Parameter of ET is ', ET_grid.best_params_)\n",
    "print('Best Score of ET is ', ET_grid.best_score_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "Bagging_param = {'max_samples':[0.8, 0.9, 1.0],\n",
    "                 'max_features':[0.8, 0.9, 1.0]}\n",
    "\n",
    "Bagging_grid = GridSearchCV(updated_regressor_objects['Bagging'],\n",
    "                       param_grid=Bagging_param,\n",
    "                       cv=10)\n",
    "Bagging_grid.fit(X ,y)\n",
    "\n",
    "print('Best Parameter of Bagging is ', Bagging_grid.best_params_)\n",
    "print('Best Score of Bagging is ', Bagging_grid.best_score_)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "SVR_param = {'gamma': ['scale', 'auto'],\n",
    "             'C': [18, 20, 22],\n",
    "             'epsilon': [0.01, 0.05]\n",
    "             }\n",
    "\n",
    "SVR_grid = GridSearchCV(updated_regressor_objects['SVR'],\n",
    "                       param_grid=SVR_param,\n",
    "                       cv=10)\n",
    "SVR_grid.fit(X ,y)\n",
    "\n",
    "print('Best Parameter of SVR is ', SVR_grid.best_params_)\n",
    "print('Best Score of SVR is ', SVR_grid.best_score_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "kNN_param = {'weights': ['uniform', 'distance'],\n",
    "             'algorithm': ['ball_tree', 'kd_tree', 'brute'],\n",
    "             'p': [1, 2, 3, 4]}\n",
    "\n",
    "kNN_grid = GridSearchCV(updated_regressor_objects['kNN'],\n",
    "                       param_grid=kNN_param,\n",
    "                       cv=10)\n",
    "kNN_grid.fit(X ,y)\n",
    "\n",
    "print('Best Parameter of kNN is ', kNN_grid.best_params_)\n",
    "print('Best Score of kNN is ', kNN_grid.best_score_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "XGB_param = {#'learning_rate ': [0.01, 0.1, 0.3],\n",
    "             'gamma': [0, 1, 3, 5, 7, 9],\n",
    "             #'max_depth ': [3, 6, 9],\n",
    "             #'min_child_weight ': [0, 1, 10],\n",
    "             }\n",
    "\n",
    "XGB_grid = GridSearchCV(updated_regressor_objects['XGB'],\n",
    "                       param_grid=XGB_param,\n",
    "                       cv=10)\n",
    "XGB_grid.fit(X ,y)\n",
    "\n",
    "print('Best Parameter of XGB is ', XGB_grid.best_params_)\n",
    "print('Best Score of XGB is ', XGB_grid.best_score_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "## 3.3. (Neural Network Intelligence is Implement in  Another File)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "## 4. 10-Fold CV\n",
    "#### Define Ploting Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "def plot_scatter(predict, experiment, regressor,line_split, property, outdir):\n",
    "    # define density plot\n",
    "    xy = np.vstack([experiment, predict])\n",
    "    z = gaussian_kde(xy)(xy)\n",
    "    # define stright lines\n",
    "    num_list = []\n",
    "    num_list.extend(predict)\n",
    "    num_list.extend(experiment)\n",
    "    mini=math.floor(min(num_list))\n",
    "    maxi=math.ceil(max(num_list))\n",
    "    fig=plt.figure(figsize=(12,10))\n",
    "    ax = fig.add_subplot()\n",
    "    plt.scatter(experiment,predict,s=20, c=z, cmap='Spectral')\n",
    "    line = mlines.Line2D([mini,maxi], [mini, maxi], color='red')\n",
    "    line1 = mlines.Line2D([mini,maxi - line_split], [mini + line_split, maxi], color='red')\n",
    "    line2 = mlines.Line2D([mini + line_split,maxi], [mini, maxi - line_split], color='red')\n",
    "    ax.add_line(line)\n",
    "    ax.add_line(line1)\n",
    "    ax.add_line(line2)\n",
    "    plt.title(\"Experimental by Predicted {} using \".format(property) + regressor + '\\nin 10 Fold Validation')\n",
    "    plt.xlabel(\"Experimental {}\".format(property))\n",
    "    plt.ylabel(\"Predicted {}\".format(property))\n",
    "    plt.xlim(mini,maxi)\n",
    "    plt.ylim(mini,maxi)\n",
    "    #plt.grid(True)\n",
    "    plt.colorbar()\n",
    "    plt.tight_layout()\n",
    "    outdir = outdir + '/{}.png'.format(regressor)\n",
    "    plt.savefig(outdir, dpi=328)\n",
    "    plt.show()\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "#### Define Algorithms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "algorithms = ['RF', 'ET', 'Bagging', 'SVR','kNN','XGB', 'MLP', 'NN']\n",
    "# Define a code that run through algorithms of sklearn and result in the same format of output data\n",
    "# sklearn algorithms and XGB\n",
    "regressor_objects = {\n",
    "                     # Ensemble regressors\n",
    "                     'RF': ensemble.RandomForestRegressor(n_estimators=80, n_jobs=-1,\n",
    "                                                          max_features='auto',\n",
    "                                                          min_samples_leaf=1,\n",
    "                                                          min_samples_split=2),\n",
    "\n",
    "                     'ET': ensemble.ExtraTreesRegressor(n_estimators=90,n_jobs=-1,\n",
    "                                                        max_features='auto',\n",
    "                                                        min_samples_leaf=1,\n",
    "                                                        min_samples_split=2),\n",
    "\n",
    "                     'Bagging': ensemble.BaggingRegressor(n_estimators=100, n_jobs=-1,\n",
    "                                                          max_features=0.9,\n",
    "                                                          max_samples=1.0),\n",
    "                     # SVR and k-NN\n",
    "                     'SVR': svm.SVR(gamma='scale',\n",
    "                                    C=20,\n",
    "                                    epsilon=0.01),\n",
    "\n",
    "                     'kNN': neighbors.KNeighborsRegressor(n_neighbors=8,n_jobs=-1,\n",
    "                                                          algorithm='ball_tree',\n",
    "                                                          p=3,\n",
    "                                                          weights='distance'),\n",
    "\n",
    "                     # XGB\n",
    "                     'XGB': XGBRegressor(n_estimators=100,n_jobs=-1),\n",
    "\n",
    "                     # simple NN\n",
    "                     'MLP': neural_network.MLPRegressor((256,256,256))\n",
    "                     }\n",
    "\n",
    "\n",
    "# define a NN from tensorflow\n",
    "def nn(input_shape):\n",
    "    model = keras.Sequential([\n",
    "        # Hidden Layer 1\n",
    "        layers.Dense(256, activation='relu', input_shape=input_shape),\n",
    "        layers.BatchNormalization(),\n",
    "        layers.Dropout(rate=0.24),\n",
    "\n",
    "        # Hidden Layer 2\n",
    "        layers.Dense(256, activation='relu'),\n",
    "        layers.BatchNormalization(),\n",
    "        layers.Dropout(rate=0.24),\n",
    "\n",
    "        # Hidden Layer 3\n",
    "        layers.Dense(256, activation='relu'),\n",
    "        layers.BatchNormalization(),\n",
    "        layers.Dropout(rate=0.24),\n",
    "\n",
    "        # Output Layer\n",
    "        layers.Dense(1)\n",
    "    ])\n",
    "\n",
    "    optimizer = tf.keras.optimizers.Adam(0.0001)\n",
    "    # loss can be mse or rmse\n",
    "    model.compile(\n",
    "                loss='mse',\n",
    "                #loss=rmse_loss,\n",
    "                optimizer=optimizer,\n",
    "                metrics=['mae', 'mse'])\n",
    "\n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "#### 10 Fold-CV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "data = pd.read_csv(r'input_csv_path')\n",
    "outdir = r'output_path'\n",
    "\n",
    "excel_dir = outdir + 'output_outliers_xlsx_path'\n",
    "writer = pd.ExcelWriter(excel_dir)\n",
    "\n",
    "# number of descriptors for covariance: 20, 90, 160, 360, 640\n",
    "X = prepare_data(data.iloc[:,5:], prepro='quantile_transform')\n",
    "y = data['IE']\n",
    "kf = KFold(n_splits=10)\n",
    "kf.get_n_splits(X)\n",
    "\n",
    "# For data storage\n",
    "train_cv_df = pd.DataFrame(columns=['Regressor', 'R^2', 'RMSE', 'Fold Number'])\n",
    "test_cv_df = pd.DataFrame(columns=['Regressor', 'R^2', 'RMSE', 'Fold Number'])\n",
    "\n",
    "for algorithm in algorithms:\n",
    "    fold_number = 0\n",
    "    y_predicted_list = []\n",
    "    y_experimental_list = []\n",
    "    # loop for CV of the tf NN object\n",
    "    if algorithm == 'NN':\n",
    "        outliers = pd.DataFrame(columns=['CAS Name','InChI','CAS Link','smiles','IE', 'Predicted IE'])\n",
    "        for train_index, test_index in kf.split(X):\n",
    "            fold_number += 1\n",
    "            model = nn([len(data.iloc[:,5:].columns)]) # data.iloc[:,5:].columns\n",
    "            X_train, X_test = X[train_index], X[test_index]\n",
    "            y_train, y_test = y[train_index], y[test_index]\n",
    "            splited_data = [X_train, X_test, y_train, y_test]\n",
    "\n",
    "            early_stop = keras.callbacks.EarlyStopping(monitor='val_loss', patience=30)\n",
    "\n",
    "            model.fit(\n",
    "                X_train, y_train,\n",
    "                batch_size=32, # batch size is a hyper-opt\n",
    "                epochs=1000, validation_split=0.1, verbose=0,\n",
    "                callbacks=[early_stop])\n",
    "\n",
    "            y_train_prediction = model.predict(X_train).flatten()\n",
    "            # write train metrics\n",
    "            r2_train = np.square(pearsonr(y_train, y_train_prediction)[0])\n",
    "            rmse_train = rmse(y_train, y_train_prediction)\n",
    "            train_new_row = pd.DataFrame([[algorithm, r2_train, rmse_train, fold_number]], columns=['Regressor', 'R^2', 'RMSE', 'Fold Number'])\n",
    "            train_cv_df = train_cv_df.append(train_new_row, ignore_index=True)\n",
    "\n",
    "            # Prediction by Neural Network\n",
    "            y_test_prediction = model.predict(X_test).flatten()\n",
    "            # write test metrics\n",
    "            r2_test = np.square(pearsonr(y_test, y_test_prediction)[0])\n",
    "            rmse_test = rmse(y_test, y_test_prediction)\n",
    "            metrics = [r2_train, rmse_train, r2_test, rmse_test, y_test_prediction]\n",
    "            test_new_row = pd.DataFrame([[algorithm, r2_test, rmse_test, fold_number]], columns=['Regressor', 'R^2', 'RMSE', 'Fold Number'])\n",
    "            test_cv_df = test_cv_df.append(test_new_row, ignore_index=True)\n",
    "\n",
    "            y_predicted_list.extend(y_test_prediction)\n",
    "            y_experimental_list.extend(y_test)\n",
    "            print(algorithm, 'at fold ', fold_number)\n",
    "            for i in range(len(test_index)):\n",
    "                ind_name = test_index[i]\n",
    "                #if np.abs(metrics[2][i] - y_test[ind_name]) > 1.: # set outlier threshold\n",
    "                mol = data.iloc[:,:5].iloc[[ind_name]].values[0]\n",
    "                new_row = pd.DataFrame([[mol[0], mol[1], mol[2], mol[3], mol[4], metrics[4][i]]], columns=['CAS Name','InChI','CAS Link','smiles','IE', 'Predicted IE'])\n",
    "                outliers = outliers.append(new_row, ignore_index=True)\n",
    "        ots = outliers\n",
    "        ots.to_excel(writer, sheet_name=algorithm)\n",
    "        plot_scatter(y_predicted_list, y_experimental_list, algorithm, 1, 'IE',outdir)\n",
    "\n",
    "    else:\n",
    "        outliers = pd.DataFrame(columns=['CAS Name','InChI','CAS Link','smiles','IE', 'Predicted IE'])\n",
    "        # loop for CVs of sklearn and xgb objects\n",
    "        for train_index, test_index in kf.split(X):\n",
    "            fold_number += 1\n",
    "\n",
    "            X_train, X_test = X[train_index], X[test_index]\n",
    "            y_train, y_test = y[train_index], y[test_index]\n",
    "            splited_data = [X_train, X_test, y_train, y_test]\n",
    "\n",
    "            metrics = ml_model(splited_data, regressor_objects[ algorithm])\n",
    "            # [R2_train, RMSE_train, R2_test, RMSE_test, test_prediction]\n",
    "            # write train metrics\n",
    "            train_new_row = pd.DataFrame([[algorithm, metrics[0], metrics[1], fold_number]], columns=['Regressor', 'R^2', 'RMSE', 'Fold Number'])\n",
    "            train_cv_df = train_cv_df.append(train_new_row, ignore_index=True)\n",
    "\n",
    "            # write test metrics\n",
    "            test_new_row = pd.DataFrame([[algorithm, metrics[2], metrics[3], fold_number]], columns=['Regressor', 'R^2', 'RMSE', 'Fold Number'])\n",
    "            test_cv_df = test_cv_df.append(test_new_row, ignore_index=True)\n",
    "\n",
    "            y_predicted_list.extend(metrics[4])\n",
    "            y_experimental_list.extend(y_test)\n",
    "            print(algorithm, 'at fold ', fold_number)\n",
    "            \n",
    "            for i in range(len(test_index)):\n",
    "                ind_name = test_index[i]\n",
    "                #if np.abs(metrics[2][i] - y_test[ind_name]) > 1.: # set outlier threshold\n",
    "                mol = data.iloc[:,:5].iloc[[ind_name]].values[0]\n",
    "                new_row = pd.DataFrame([[mol[0], mol[1], mol[2], mol[3], mol[4], metrics[4][i]]], columns=['CAS Name','InChI','CAS Link','smiles','IE', 'Predicted IE'])\n",
    "                outliers = outliers.append(new_row, ignore_index=True)\n",
    "        ots = outliers\n",
    "        ots.to_excel(writer, sheet_name=algorithm)\n",
    "        plot_scatter(y_predicted_list, y_experimental_list, algorithm, 1, 'IE',outdir)\n",
    "\n",
    "writer.save()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "train_dir = outdir + 'output_train_excel_path'\n",
    "train_cv_df.to_excel(train_dir)\n",
    "train_cv_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "test_dir = outdir + 'output_test_excel_path'\n",
    "test_cv_df.to_excel(test_dir)\n",
    "test_cv_df\n"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "e697c487eee6592dcc148095488cb6a0a9935c2a44d966f190886294b135e553"
  },
  "kernelspec": {
   "display_name": "Python 3.7.0 64-bit ('iepaper': conda)",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
